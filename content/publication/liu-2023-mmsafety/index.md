---
title: 'MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models'
authors:
- admin
- Yichen Zhu (Equal first author)
- Jindong Gu
- Yunshi Lan
- Chao Yang
- Yu Qiao
# Author notes (optional)
author_notes:
- 'Equal contribution'
- 'Equal contribution'
- ''
- 'Corresponding author'
- 'Corresponding author'
- ''
date: '2023-11-29'
publishDate: '2023-11-29T07:30:02.966893Z'
publication_types:
- paper-conference
publication: '*ECCV*'

featured: true
links:
- name: Project Page
  url: 'https://isxinliu.github.io/Project/MM-SafetyBench/'
# - name: ECCV 2024
#   url: ''
url_pdf: https://arxiv.org/pdf/2311.17600v2
url_code: 'https://github.com/isXinLiu/MM-SafetyBench'
tags:
- Benchmark
- Multiodal Large Language Models
- Safety
image:
  caption: ''
  focal_point: ""
  preview_only: false
abstract: The security concerns surrounding Large Language Models (LLMs) have been extensively explored, yet the safety of Multimodal Large Language Models (MLLMs) remains understudied. In this paper, we observe that Multimodal Large Language Models (MLLMs) can be easily compromised by query-relevant images, as if the text query itself were malicious. To address this, we introduce MM-SafetyBench, a comprehensive framework designed for conducting safety-critical evaluations of MLLMs against such image-based manipulations. We have compiled a dataset comprising 13 scenarios, resulting in a total of 5,040 text-image pairs. Our analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by our approach, even when the equipped LLMs have been safety-aligned. In response, we propose a straightforward yet effective prompting strategy to enhance the resilience of MLLMs against these types of attacks. Our work underscores the need for a concerted effort to strengthen and enhance the safety measures of open-source MLLMs against potential malicious exploits.
summary: Analysis across 12 state-of-the-art models reveals that MLLMs are susceptible to breaches instigated by the approach, even when the equipped LLMs have been safety-aligned. Furthermore, this work proposes a straightforward yet effective prompting strategy to enhance the resilience of MLLMs against these types of attacks.
---
