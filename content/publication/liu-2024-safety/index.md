---
title: Safety of Multimodal Large Language Models on Images and Text
authors:
- admin
- Yichen Zhu
- Yunshi Lan
- Chao Yang
- Yu Qiao
date: '2024-02-01'
publishDate: '2024-02-01T07:22:55.276756Z'
publication_types:
- article-journal
publication: '*arXiv preprint arXiv:2402.00357*'
featured: true
links:
- name: Talk
  url: 'https://mp.weixin.qq.com/s/xkBAkmhs4qGUN7dkRO95Hw'
url_pdf: https://arxiv.org/pdf/2402.00357
url_code: 'https://github.com/isXinLiu/Awesome-MLLM-Safety'
tags:
- IJCAI 2024 Survey Track Accepted
- Multiodal Large Language Models
- Safety
- Survey
image:
  caption: ''
  focal_point: ""
  preview_only: false
abstract: Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios. In this paper, we systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text. We begin with introducing the overview of MLLMs on images and text and understanding of safety, which helps researchers know the detailed scope of our survey. Then, we review the evaluation datasets and metrics for measuring the safety of MLLMs. Next, we comprehensively present attack and defense techniques related to MLLMs' safety. Finally, we analyze several unsolved issues and discuss promising research directions.
summary: This paper systematically surveys current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text.
---
